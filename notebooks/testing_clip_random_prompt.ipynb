{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Estrae la cartella di input nella cartella local di Colab"
      ],
      "metadata": {
        "id": "elcpnhncvPoC"
      },
      "id": "elcpnhncvPoC"
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/inputs.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O0Ws1d1UVo0O",
        "outputId": "61b8eba9-8951-4fee-a3b2-7d7da65d06da"
      },
      "id": "O0Ws1d1UVo0O",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/inputs.zip\n",
            "   creating: inputs/\n",
            "   creating: inputs/clean/\n",
            "   creating: inputs/clean/chair/\n",
            "  inflating: inputs/clean/chair/sedia1.jpg  \n",
            "  inflating: inputs/clean/chair/sedia3.jpg  \n",
            "   creating: inputs/clean/cup/\n",
            "  inflating: inputs/clean/cup/tazza1.jpg  \n",
            "  inflating: inputs/clean/cup/tazzaa2.jpg  \n",
            "   creating: inputs/clean/lamp/\n",
            "  inflating: inputs/clean/lamp/lampada1.jpg  \n",
            "  inflating: inputs/clean/lamp/lampada2.jpg  \n",
            "   creating: inputs/clean/sofa/\n",
            "  inflating: inputs/clean/sofa/divano1.jpg  \n",
            "  inflating: inputs/clean/sofa/divano3.jpg  \n",
            "   creating: inputs/clean/table/\n",
            "  inflating: inputs/clean/table/tavolo1.jpg  \n",
            "  inflating: inputs/clean/table/tavolo2.jpg  \n",
            "   creating: inputs/clean/vase/\n",
            "  inflating: inputs/clean/vase/vaso2.jpg  \n",
            "  inflating: inputs/clean/vase/vaso3.jpg  \n",
            "   creating: inputs/real/\n",
            "   creating: inputs/real/chair/\n",
            "  inflating: inputs/real/chair/sedia1.jpg  \n",
            "  inflating: inputs/real/chair/sedia2.jpg  \n",
            "   creating: inputs/real/cup/\n",
            "  inflating: inputs/real/cup/tazza1.jpg  \n",
            "  inflating: inputs/real/cup/tazza3.jpg  \n",
            "   creating: inputs/real/lamp/\n",
            "  inflating: inputs/real/lamp/lampada2.jpg  \n",
            "  inflating: inputs/real/lamp/lampada3.jpeg  \n",
            "   creating: inputs/real/sofa/\n",
            "  inflating: inputs/real/sofa/divano1.jpg  \n",
            "  inflating: inputs/real/sofa/divano3.jpg  \n",
            "   creating: inputs/real/table/\n",
            "  inflating: inputs/real/table/tavolo1.jpg  \n",
            "  inflating: inputs/real/table/tavolo3.jpg  \n",
            "   creating: inputs/real/vase/\n",
            "  inflating: inputs/real/vase/vaso1.jpg  \n",
            "  inflating: inputs/real/vase/vaso2.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installazione dei principali framework e package"
      ],
      "metadata": {
        "id": "eG525NjQvct0"
      },
      "id": "eG525NjQvct0"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install open-clip-torch\n",
        "!pip install jaxtyping einops open-clip-torch ftfy regex timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RI75mZ8c0BK",
        "outputId": "148cbfba-33a9-4c48-9082-4686445e53a0",
        "collapsed": true
      },
      "id": "9RI75mZ8c0BK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting open-clip-torch\n",
            "  Downloading open_clip_torch-3.2.0-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (0.23.0+cu126)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (2024.11.6)\n",
            "Collecting ftfy (from open-clip-torch)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (0.35.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (0.6.2)\n",
            "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (1.0.20)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open-clip-torch) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open-clip-torch) (0.2.14)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch) (1.1.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->open-clip-torch) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open-clip-torch) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open-clip-torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open-clip-torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (2025.10.5)\n",
            "Downloading open_clip_torch-3.2.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy, open-clip-torch\n",
            "Successfully installed ftfy-6.3.1 open-clip-torch-3.2.0\n",
            "Collecting jaxtyping\n",
            "  Downloading jaxtyping-0.3.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Requirement already satisfied: open-clip-torch in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (0.35.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (0.6.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch) (1.1.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->open-clip-torch) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open-clip-torch) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open-clip-torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open-clip-torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (2025.10.5)\n",
            "Downloading jaxtyping-0.3.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, jaxtyping\n",
            "Successfully installed jaxtyping-0.3.3 wadler-lindig-0.1.7\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "e933fecd45bf98b0"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import os,json\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:256\"\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torchvision.transforms.v2.functional as tvf\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sphinx.ext.viewcode import OUTPUT_DIRNAME\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "#Todo togliere il commmento\n",
        "from einops import rearrange\n"
      ],
      "id": "e933fecd45bf98b0"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.use_deterministic_algorithms(False)\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "lE3clQQQsOD1"
      },
      "id": "lE3clQQQsOD1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6914e49d69d7390f"
      },
      "cell_type": "markdown",
      "source": [
        "Impostazione del dispositivo di calcolo"
      ],
      "id": "6914e49d69d7390f"
    },
    {
      "metadata": {
        "id": "ae9b610766d3c7ca"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "id": "ae9b610766d3c7ca"
    },
    {
      "metadata": {
        "id": "87b4eee4e608979"
      },
      "cell_type": "markdown",
      "source": [
        "Clip_model per ottenere gli score delle generazioni"
      ],
      "id": "87b4eee4e608979"
    },
    {
      "metadata": {
        "id": "d599a524b67dae63"
      },
      "cell_type": "markdown",
      "source": [
        "Definizioni dei Path\n"
      ],
      "id": "d599a524b67dae63"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "181bec33ba911a97",
        "outputId": "df3a2198-db2f-494f-8c71-50574c8b5721"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE_DATA_DIR    = /content\n",
            "INPUTS_PATH      = /content/inputs\n",
            "OUT_DIR          = /content/outputs\n",
            "SCORES_FILE_PATH = /content/outputs/clip_score.txt\n"
          ]
        }
      ],
      "execution_count": null,
      "source": [
        "from pathlib import Path\n",
        "\n",
        "BASE_DATA_DIR=Path(\"/content\")\n",
        "INPUTS_PATH=BASE_DATA_DIR/\"inputs\"\n",
        "OUT_DIR=Path(\"/content/outputs\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "SCORES_FILE_PATH=OUT_DIR/\"clip_score.txt\"\n",
        "\n",
        "print('BASE_DATA_DIR    =', BASE_DATA_DIR)\n",
        "print('INPUTS_PATH      =', INPUTS_PATH)\n",
        "print('OUT_DIR          =', OUT_DIR)\n",
        "print('SCORES_FILE_PATH =', SCORES_FILE_PATH)"
      ],
      "id": "181bec33ba911a97"
    },
    {
      "metadata": {
        "id": "5c2ef5ddb86d062"
      },
      "cell_type": "markdown",
      "source": [
        "## Import del progetto\n",
        "Questo import include sia token-opt sia codice personale per generazione dei prompt (PrommptConfigurator), estrattore di immagini dalla cartella di input (ImageExtractor) e ImageSaver per creare l'immagine di output (immagine originale+immagine generata)"
      ],
      "id": "5c2ef5ddb86d062"
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os, pathlib, sys, zipfile, shutil, subprocess\n",
        "\n",
        "!git clone https://github.com/niccobarto/token-opt-generator.git\n",
        "\n",
        "REPO_DIR = Path('/content/token-opt-generator')\n",
        "PERSONAL_PROJECT_DIR=Path('content/token-opt-generator/tester')\n",
        "TOKEN_OPT_DIR= REPO_DIR/\"token-opt\"\n",
        "sys.path.insert(0,str(TOKEN_OPT_DIR))\n",
        "sys.path.insert(0,str(REPO_DIR))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC2-cpFZLWkh",
        "outputId": "5bf2fa91-d294-4a78-ae30-051773c5a6c6"
      },
      "id": "BC2-cpFZLWkh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'token-opt-generator'...\n",
            "remote: Enumerating objects: 152, done.\u001b[K\n",
            "remote: Counting objects: 100% (152/152), done.\u001b[K\n",
            "remote: Compressing objects: 100% (130/130), done.\u001b[K\n",
            "remote: Total 152 (delta 31), reused 134 (delta 19), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (152/152), 26.73 MiB | 19.90 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfb2c5c2ac03abb1",
        "outputId": "b73ecd84-27de-4faf-e189-780ddd4065d3"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token-Opt importato correttamente\n",
            "True\n",
            "Tester importato correttamente\n",
            "True\n"
          ]
        }
      ],
      "execution_count": null,
      "source": [
        "TTO_AVAILABLE = True\n",
        "try:\n",
        "    from tto.test_time_opt import(\n",
        "        TestTimeOpt,\n",
        "        TestTimeOptConfig,\n",
        "        CLIPObjective,\n",
        "    )\n",
        "    print(\"Token-Opt importato correttamente\")\n",
        "except Exception as e:\n",
        "    TTO_AVAILABLE = False\n",
        "    print(\"Errore nell'importazione di Token-Opt:\", e)\n",
        "\n",
        "print(TTO_AVAILABLE)\n",
        "\n",
        "TESTER_AVAILABLE=True\n",
        "try:\n",
        "  from tester.ImageExtractor import Extractor\n",
        "  import tester.ImageSaver as ImageSaver\n",
        "  from tester.PromptConfigurator import PromptConfigurator\n",
        "  print(\"Tester importato correttamente\")\n",
        "except Exception as e:\n",
        "  TESTER_AVAILABLE=False\n",
        "  print(\"Errore nell'importazione di Tester\",e)\n",
        "print(TESTER_AVAILABLE)\n",
        "\n"
      ],
      "id": "dfb2c5c2ac03abb1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per trasformare immagine in tensore e tensore in immagine"
      ],
      "metadata": {
        "id": "DeR8J8jLmPRg"
      },
      "id": "DeR8J8jLmPRg"
    },
    {
      "cell_type": "code",
      "source": [
        "def image_to_tensor(image:Image.Image, device=None):\n",
        "  img = (1. / 255.) * torch.from_numpy(\n",
        "      np.array(image).astype(np.float32)\n",
        "  ).permute(2, 0, 1)\n",
        "  img = tvf.resize(img, 256)\n",
        "  img = tvf.center_crop(img, 256)\n",
        "  img = img.unsqueeze(0)\n",
        "  if device is not None:\n",
        "      img = img.to(device)\n",
        "  return img\n",
        "\n",
        "def tensor_to_image(t: torch.Tensor) -> Image.Image:\n",
        "    # Valori attesi in [0,1], su GPU o CPU\n",
        "    t = t.detach().clamp(0, 1)\n",
        "\n",
        "    # Se arriva [C,H,W], aggiungo la dim di batch -> [1,C,H,W]\n",
        "    if t.ndim == 3:\n",
        "        t = t.unsqueeze(0)\n",
        "    elif t.ndim != 4:\n",
        "        raise ValueError(f\"tensor_to_image: tensore con {t.ndim} dimensioni non supportato\")\n",
        "\n",
        "    # [B,C,H,W] -> [H, B*W, C], uint8 su CPU\n",
        "    img = Image.fromarray(\n",
        "        rearrange((t * 255).to(dtype=torch.uint8, device=\"cpu\"), \"b c h w -> h (b w) c\").numpy()\n",
        "    )\n",
        "    return img"
      ],
      "metadata": {
        "id": "wcTR6iV2mH8W"
      },
      "id": "wcTR6iV2mH8W",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8d2aa659d509a274"
      },
      "cell_type": "markdown",
      "source": [
        "# Metodo di generazione dell'immagine\n",
        "`get_prompts()` utilizza PromptConfigurator per generare dei prompt in base alla categoria di modifica dell'immagine richiesta.\n",
        "\n",
        "`get_images()` ricerca le immagini richieste nella cartella di inputs.\n",
        "\n",
        "`run_token_opt()` esegue la generazione dell'immagine in base ad un'immagine di partenza ed il prompt associato. Il metodo trasforma l'immagine seed in tensore, la passa al token-opt e restituisce l'immagine riconvertita in RGB."
      ],
      "id": "8d2aa659d509a274"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prompts(object_choose:str,category_choose:str,number_of_prompt:int,is_clean:bool):\n",
        "  prompt_configurator=PromptConfigurator()\n",
        "  prompts=prompt_configurator.make_prompts(object_choose,\n",
        "                                          category_choose,\n",
        "                                          number_of_prompt,\n",
        "                                          is_clean,\n",
        "                                          )\n",
        "  return prompts\n",
        "\n",
        "def get_images(obj:str)->tuple[list[Image.Image],list[Image.Image]]:\n",
        "  extractor=Extractor(INPUTS_PATH)\n",
        "  obj,clean_paths,real_paths=extractor.extract_paths(obj)\n",
        "  obj,clean_images,real_images=extractor.extract_images(obj,clean_paths,real_paths)\n",
        "  return clean_images,real_images\n",
        "\n",
        "def run_token_opt(seed_img:Image.Image,prompt:str,cfg:TestTimeOptConfig)->Image.Image:\n",
        "    \"\"\"\n",
        "    Esegue Token_Opt su una singola immagine+prompt. Se Token-Opt non è importato correttamente si restituisce l'immagine seed.\n",
        "    \"\"\"\n",
        "    if not TTO_AVAILABLE:\n",
        "        print(\"Token-Opt non è disponibile, restituisco l'immagine seed\")\n",
        "        return seed_img\n",
        "\n",
        "    seed_tensor=image_to_tensor(seed_img,device=\"cuda\")\n",
        "    #Definizione dell'obiettivo CLIP\n",
        "    objective.prompt=prompt\n",
        "    print(f\"Prompt {prompt} associato all'obiettivo\")\n",
        "    tensor_out,loss=tto(seed=seed_tensor)\n",
        "    clip_score=(-loss.detach().mean()).item()\n",
        "    return tensor_to_image(tensor_out),clip_score"
      ],
      "metadata": {
        "id": "aROlQ6UxQ13L"
      },
      "id": "aROlQ6UxQ13L",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cc082c4cb3e2b777"
      },
      "cell_type": "markdown",
      "source": [
        "# Loop principale"
      ],
      "id": "cc082c4cb3e2b777"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creazione del TestTimeOptConfig, ovvero la configurazione per il TestTimeOpt.\n",
        "\n",
        "Creazione dell'oggetto TestTimeOpt e dell'obiettivo CLIPObjective"
      ],
      "metadata": {
        "id": "yPEEPdTFxW0f"
      },
      "id": "yPEEPdTFxW0f"
    },
    {
      "cell_type": "code",
      "source": [
        "steps=301\n",
        "cfg=TestTimeOptConfig(\n",
        "        num_iter=steps,\n",
        "        ema_decay=0.98,\n",
        "        lr=1e-1,\n",
        "        enable_amp=True,\n",
        "        reg_weight=0.025,\n",
        "        token_noise=0.3,\n",
        "        reg_type=\"seed\",\n",
        "        vae_deterministic_sampling=False,\n",
        "\n",
        "    )\n",
        "objective=CLIPObjective(num_augmentations=8, cfg_scale=1.2)\n",
        "tto=TestTimeOpt(cfg,objective).to(device)\n"
      ],
      "metadata": {
        "id": "8R-G0oZ0W4LG"
      },
      "id": "8R-G0oZ0W4LG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il metodo `process_images()` riceve una lista di immagini ed una lista di prompt. Per ogni immagine si esegue la generazione di token-opt con ogni prompt presente in `prompts`. Nel metodo una volta ricevuto l'immagine di output la valuto con ClipScore per misurare quando l'immagine assomiglia al prompt.\n",
        "Restituisco per ogni prompt eseguito lo score associato a tale prompt."
      ],
      "metadata": {
        "id": "tm08vMM2x6Ce"
      },
      "id": "tm08vMM2x6Ce"
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images(images:list[Image.Image],image_type:str,obj:str,category:str):\n",
        "  is_clean=True if image_type==\"clean\" else False\n",
        "  clip_prompt_scores=[]\n",
        "  number_dir=1\n",
        "  for i in range(len(images)):\n",
        "    number=1\n",
        "    prompts=get_prompts(obj,category,prompts_per_image,is_clean)\n",
        "    for prompt in prompts:\n",
        "      output,clip_score=run_token_opt(images[i],prompt,cfg)\n",
        "      clip_prompt_scores.append((clip_score,prompt))\n",
        "      ImageSaver.create_side_by_side_with_caption(\n",
        "              images[i],\n",
        "              output,\n",
        "              prompt=prompt,\n",
        "              value=clip_score,\n",
        "              out_path=OUT_DIR/f\"{category}/{image_type}/{obj}/{obj}{number_dir}/{obj}{number}.jpg\"\n",
        "          )\n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()\n",
        "      number=number+1\n",
        "    number_dir=number_dir+1\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  return clip_prompt_scores"
      ],
      "metadata": {
        "id": "CpVQ0JkEkfod"
      },
      "id": "CpVQ0JkEkfod",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il metodo `process_categoty()` esegue su ogni oggetto deciso il testing di generazione sulla categoria `categoty` passata come parametro.\n",
        "\n",
        "Oltre che scrivere i risultati su un file di testo, chiama il metodo `process_images()` su una lista di immagini raffiguranti lo stesso oggetto (e per oggetti con sfondo bianco e in contesti veri)"
      ],
      "metadata": {
        "id": "BqhRnzsTWYcS"
      },
      "id": "BqhRnzsTWYcS"
    },
    {
      "cell_type": "code",
      "source": [
        "def process_category(category:str):\n",
        "  category_real_score=[]\n",
        "  category_clean_score=[]\n",
        "\n",
        "  with open(str(SCORES_FILE_PATH),\"a\",encoding=\"utf-8\") as f:\n",
        "    f.write(f\"-------- {category} prompt testing --------\\n\")\n",
        "\n",
        "  for obj in objects:\n",
        "    real_scores=[]\n",
        "    clean_scores=[]\n",
        "    clean_images,real_images=get_images(obj)\n",
        "\n",
        "    print(f\"Processing real {obj} in {category} generation\")\n",
        "\n",
        "    #REAL IMAGES PROCESSING\n",
        "    real_prompt_scores=process_images(real_images,\"real\",obj,category)\n",
        "    real_rows=[]\n",
        "    for score, prompt in real_prompt_scores:\n",
        "        real_scores.append(score)\n",
        "        real_rows.append(f\"{category.upper()} REAL IMAGES generation. Prompt: {prompt}, Score: {score}\\n\")\n",
        "    print(f\"Done real {obj} on {category} generation\")\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"Processing clean {obj} in {category} generation\")\n",
        "\n",
        "    #CLEAN IMAGES PROCESSING\n",
        "    clean_prompt_scores=process_images(clean_images,\"clean\",obj,category)\n",
        "    clean_rows=[]\n",
        "    for score, prompt in clean_prompt_scores:\n",
        "        clean_scores.append(score)\n",
        "        clean_rows.append(f\"{category.upper()} CLEAN IMAGES generation. Prompt: {prompt}, Score: {score}\\n\")\n",
        "\n",
        "    print(f\"Done clean {obj} on {category} generation\")\n",
        "\n",
        "    with open(str(SCORES_FILE_PATH),\"a\",encoding=\"utf-8\") as f:\n",
        "      f.writelines(real_rows)\n",
        "      f.writelines(clean_rows)\n",
        "\n",
        "    category_real_score.extend(real_scores)\n",
        "    category_clean_score.extend(clean_scores)\n",
        "\n",
        "  print(f\"{category} generation DONE\")\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  average_real=sum(category_real_score)/len(category_real_score)\n",
        "  average_clean=sum(category_clean_score)/len(category_clean_score)\n",
        "  end_category_real_row=f\"The average ClipScore for {category} on real images is: {average_real}\\n\"\n",
        "  end_category_clean_row=f\"The average ClipScore for {category} on clean images is: {average_clean}\\n\"\n",
        "  with open(str(SCORES_FILE_PATH),\"a\",encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\")\n",
        "    f.write(end_category_real_row)\n",
        "    f.write(end_category_clean_row)\n",
        "    f.write(f\"-------- End {category} prompt testing --------\\n\")\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "gqV4JnVo4mN_"
      },
      "id": "gqV4JnVo4mN_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scegli quali oggetti e categorie di generazione testare"
      ],
      "metadata": {
        "id": "SctDeyglV0sa"
      },
      "id": "SctDeyglV0sa"
    },
    {
      "metadata": {
        "id": "a6caf72cc73143ee",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b107ea8-5feb-47e5-df14-6feb42704351"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing real chair in change_material generation\n",
            "Prompt a photo of a chair made of marble stone without changing background, high detail associato all'obiettivo\n",
            "Prompt a photo of a chair made of black metal without changing background, high detail associato all'obiettivo\n",
            "Prompt a photo of a chair made of polished steel without changing background, high detail associato all'obiettivo\n",
            "Prompt a photo of a chair made of brushed aluminum without changing background, high detail associato all'obiettivo\n",
            "Done real chair on change_material generation\n",
            "Processing clean chair in change_material generation\n",
            "Prompt a photo of a chair made of black metal on a white background, high detail associato all'obiettivo\n",
            "Prompt a photo of a chair made of leather on a white background, high detail associato all'obiettivo\n",
            "Prompt a photo of a chair made of carbon fiber on a white background, high detail associato all'obiettivo\n",
            "Prompt a photo of a chair made of brushed aluminum on a white background, high detail associato all'obiettivo\n",
            "Done clean chair on change_material generation\n",
            "Processing real cup in change_material generation\n",
            "Prompt a photo of a cup made of polished steel without changing background, high detail associato all'obiettivo\n",
            "Prompt a photo of a cup made of wood without changing background, high detail associato all'obiettivo\n",
            "Prompt a photo of a cup made of marble stone without changing background, high detail associato all'obiettivo\n",
            "Prompt a photo of a cup made of transparent acrylic without changing background, high detail associato all'obiettivo\n",
            "Done real cup on change_material generation\n",
            "Processing clean cup in change_material generation\n",
            "Prompt a photo of a cup made of carbon fiber on a white background, high detail associato all'obiettivo\n",
            "Prompt a photo of a cup made of black metal on a white background, high detail associato all'obiettivo\n",
            "Prompt a photo of a cup made of ceramic on a white background, high detail associato all'obiettivo\n",
            "Prompt a photo of a cup made of marble stone on a white background, high detail associato all'obiettivo\n",
            "Done clean cup on change_material generation\n",
            "Processing real lamp in change_material generation\n",
            "Prompt a photo of a lamp made of wood without changing background, high detail associato all'obiettivo\n",
            "Prompt a photo of a lamp made of wood without changing background, high detail associato all'obiettivo\n",
            "Prompt a photo of a lamp made of ceramic without changing background, high detail associato all'obiettivo\n",
            "Prompt a photo of a lamp made of carbon fiber without changing background, high detail associato all'obiettivo\n",
            "Done real lamp on change_material generation\n",
            "Processing clean lamp in change_material generation\n",
            "Prompt a photo of a lamp made of ceramic on a white background, high detail associato all'obiettivo\n",
            "Prompt a photo of a lamp made of ceramic on a white background, high detail associato all'obiettivo\n",
            "Prompt a photo of a lamp made of brushed aluminum on a white background, high detail associato all'obiettivo\n",
            "Prompt a photo of a lamp made of black metal on a white background, high detail associato all'obiettivo\n",
            "Done clean lamp on change_material generation\n",
            "Processing real sofa in change_material generation\n",
            "Prompt a photo of a sofa made of polished steel without changing background, high detail associato all'obiettivo\n",
            "Prompt a photo of a sofa made of polished steel without changing background, high detail associato all'obiettivo\n",
            "Prompt a photo of a sofa made of ceramic without changing background, high detail associato all'obiettivo\n",
            "Prompt a photo of a sofa made of wood without changing background, high detail associato all'obiettivo\n",
            "Done real sofa on change_material generation\n",
            "Processing clean sofa in change_material generation\n",
            "Prompt a photo of a sofa made of ceramic on a white background, high detail associato all'obiettivo\n"
          ]
        }
      ],
      "execution_count": null,
      "source": [
        "objects =[\"chair\",\"cup\",\"lamp\",\"sofa\",\"table\",\"vase\"]\n",
        "categoryes = [\"change_material\", \"change_color\", \"change_style\", \"add_component\"]\n",
        "prompts_per_image=2\n",
        "\n",
        "\n",
        "for category in categoryes:\n",
        "  process_category(category)"
      ],
      "id": "a6caf72cc73143ee"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprimi la cartella di output per permettere il download"
      ],
      "metadata": {
        "id": "T61zuQ_3VrOM"
      },
      "id": "T61zuQ_3VrOM"
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r outputs /content/outputs"
      ],
      "metadata": {
        "id": "VYpOcJiD0lbv",
        "collapsed": true
      },
      "id": "VYpOcJiD0lbv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cella bonus:\n",
        "svuota la VRAM"
      ],
      "metadata": {
        "id": "kzvSHTYbVdwN"
      },
      "id": "kzvSHTYbVdwN"
    },
    {
      "cell_type": "code",
      "source": [
        "# PRIMA di ricreare modelli/oggetti\n",
        "\"\"\"\n",
        "import gc, torch, os\n",
        "\n",
        "for name in [\"tto\", \"objective\"]:   # aggiungi qui eventuali altri handler GPU\n",
        "    if name in globals():\n",
        "        del globals()[name]\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n"
      ],
      "metadata": {
        "id": "lv3dWjAYo1Pu"
      },
      "id": "lv3dWjAYo1Pu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}